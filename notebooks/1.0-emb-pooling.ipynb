{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6167932",
   "metadata": {},
   "source": [
    "# Embedding Pooling Strategy Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c2215",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7367fdb7",
   "metadata": {},
   "source": [
    "### Load Embedding Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b86615",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv(\"INFLAMM_DEBATE_FM_DATA_ROOT\")\n",
    "EMBEDDINGS_BASE_DIR = Path(DATA_DIR) / \"processed\" / \"bulkformer_embeddings\"\n",
    "\n",
    "# Define the three configurations\n",
    "configs = {\n",
    "    \"human_only\": EMBEDDINGS_BASE_DIR / \"human_only\",\n",
    "    \"mouse_only\": EMBEDDINGS_BASE_DIR / \"mouse_only\",\n",
    "    \"human_ortholog_filtered\": EMBEDDINGS_BASE_DIR / \"human_ortholog_filtered\",\n",
    "}\n",
    "\n",
    "# Check which configurations exist\n",
    "available_configs = {k: v for k, v in configs.items() if v.exists()}\n",
    "print(f\"Available configurations: {list(available_configs.keys())}\")\n",
    "\n",
    "# List available embedding files in each config\n",
    "for config_name, config_dir in available_configs.items():\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    npy_files = sorted(config_dir.glob(\"*.npy\"))\n",
    "    for f in npy_files:\n",
    "        print(f\"  - {f.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a8806",
   "metadata": {},
   "source": [
    "### Helper Functions for Memory-Efficient Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffcce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_for_embeddings(embedding_path: Path):\n",
    "    \"\"\"\n",
    "    Load metadata to identify control and inflammation samples.\n",
    "    Assumes metadata is stored alongside embeddings or can be inferred from AnnData.\n",
    "    For now, we'll need to load the corresponding AnnData to get sample labels.\n",
    "    \"\"\"\n",
    "    # Extract dataset name from embedding filename\n",
    "    # Format: {dataset}_transcriptome_embeddings.npy\n",
    "    dataset_name = embedding_path.stem.replace(\"_transcriptome_embeddings\", \"\")\n",
    "    \n",
    "    # Load corresponding AnnData to get metadata\n",
    "    ann_data_dir = Path(DATA_DIR) / \"processed\" / \"anndata_cleaned\"\n",
    "    ann_data_path = ann_data_dir / f\"{dataset_name}.h5ad\"\n",
    "    \n",
    "    if not ann_data_path.exists():\n",
    "        # Try alternative locations\n",
    "        ann_data_dir_alt = Path(DATA_DIR) / \"processed\" / \"anndata_orthologs\"\n",
    "        ann_data_path = ann_data_dir_alt / f\"{dataset_name}_orthologs.h5ad\"\n",
    "    \n",
    "    if ann_data_path.exists():\n",
    "        import anndata as ad\n",
    "        adata = ad.read_h5ad(ann_data_path)\n",
    "        return adata.obs\n",
    "    else:\n",
    "        print(f\"Warning: Could not find AnnData for {dataset_name}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_sample_indices(metadata, n_samples_per_group=15):\n",
    "    \"\"\"\n",
    "    Get indices for control and inflammation samples.\n",
    "    Assumes metadata has a column indicating control/inflammation status.\n",
    "    \"\"\"\n",
    "    # Try common column names for status\n",
    "    status_cols = [\"takao_status\", \"status\", \"condition\", \"group\", \"inflammation\"]\n",
    "    status_col = None\n",
    "    for col in status_cols:\n",
    "        if col in metadata.columns:\n",
    "            status_col = col\n",
    "            break\n",
    "    \n",
    "    if status_col is None:\n",
    "        print(f\"Warning: Could not find status column. Available: {metadata.columns.tolist()}\")\n",
    "        # Return first n_samples_per_group*2 samples as fallback\n",
    "        return np.arange(n_samples_per_group * 2)\n",
    "    \n",
    "    # Get control and inflammation indices\n",
    "    if \"control\" in status_col.lower() or \"takao\" in status_col.lower():\n",
    "        # Handle takao_status format\n",
    "        control_mask = metadata[status_col].str.contains(\"control\", case=False, na=False)\n",
    "        inflam_mask = metadata[status_col].str.contains(\"inflam\", case=False, na=False)\n",
    "    else:\n",
    "        # Generic handling\n",
    "        unique_vals = metadata[status_col].unique()\n",
    "        if len(unique_vals) >= 2:\n",
    "            control_mask = metadata[status_col] == unique_vals[0]\n",
    "            inflam_mask = metadata[status_col] == unique_vals[1]\n",
    "        else:\n",
    "            # Fallback: split by index\n",
    "            n_total = len(metadata)\n",
    "            return np.arange(min(n_samples_per_group * 2, n_total))\n",
    "    \n",
    "    control_indices = np.where(control_mask)[0]\n",
    "    inflam_indices = np.where(inflam_mask)[0]\n",
    "    \n",
    "    # Sample up to n_samples_per_group from each\n",
    "    n_control = min(n_samples_per_group, len(control_indices))\n",
    "    n_inflam = min(n_samples_per_group, len(inflam_indices))\n",
    "    \n",
    "    selected_control = np.random.choice(control_indices, size=n_control, replace=False)\n",
    "    selected_inflam = np.random.choice(inflam_indices, size=n_inflam, replace=False)\n",
    "    \n",
    "    return np.sort(np.concatenate([selected_control, selected_inflam]))\n",
    "\n",
    "\n",
    "def load_embeddings_subset(embedding_path: Path, sample_indices: np.ndarray):\n",
    "    \"\"\"\n",
    "    Load a subset of embeddings without loading the full array into memory.\n",
    "    Uses memory mapping for large files.\n",
    "    \"\"\"\n",
    "    # Use memory mapping to avoid loading full array\n",
    "    mmap_emb = np.load(embedding_path, mmap_mode='r')\n",
    "    \n",
    "    # Load only the selected samples\n",
    "    if len(sample_indices) > 0:\n",
    "        embeddings = mmap_emb[sample_indices]\n",
    "    else:\n",
    "        embeddings = mmap_emb[:]\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b78c07",
   "metadata": {},
   "source": [
    "### Load Embeddings with Sample Limiting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6adae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load embeddings for each configuration\n",
    "embeddings_data = {}\n",
    "\n",
    "for config_name, config_dir in available_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_embeddings = {}\n",
    "    npy_files = sorted(config_dir.glob(\"*.npy\"))\n",
    "    \n",
    "    for emb_file in npy_files:\n",
    "        dataset_name = emb_file.stem.replace(\"_transcriptome_embeddings\", \"\")\n",
    "        print(f\"\\n  Dataset: {dataset_name}\")\n",
    "        \n",
    "        # Load metadata to identify samples\n",
    "        metadata = load_metadata_for_embeddings(emb_file)\n",
    "        \n",
    "        if metadata is not None:\n",
    "            # Get sample indices (15 control + 15 inflammation)\n",
    "            sample_indices = get_sample_indices(metadata, n_samples_per_group=15)\n",
    "            print(f\"    Selected {len(sample_indices)} samples\")\n",
    "            \n",
    "            # Load embeddings subset\n",
    "            emb_subset = load_embeddings_subset(emb_file, sample_indices)\n",
    "            print(f\"    Embedding shape: {emb_subset.shape}\")\n",
    "            \n",
    "            config_embeddings[dataset_name] = {\n",
    "                \"embeddings\": emb_subset,\n",
    "                \"sample_indices\": sample_indices,\n",
    "                \"metadata\": metadata.iloc[sample_indices] if metadata is not None else None\n",
    "            }\n",
    "        else:\n",
    "            # Fallback: load first 30 samples\n",
    "            print(f\"    Warning: Loading first 30 samples (no metadata found)\")\n",
    "            emb_subset = load_embeddings_subset(emb_file, np.arange(30))\n",
    "            config_embeddings[dataset_name] = {\n",
    "                \"embeddings\": emb_subset,\n",
    "                \"sample_indices\": np.arange(30),\n",
    "                \"metadata\": None\n",
    "            }\n",
    "    \n",
    "    embeddings_data[config_name] = config_embeddings\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Loading complete!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a057f",
   "metadata": {},
   "source": [
    "### Variance Analysis: Full Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance_per_gene(embeddings):\n",
    "    \"\"\"\n",
    "    Compute variance per gene across samples.\n",
    "    embeddings: (n_samples, n_genes, embedding_dim) or (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    if len(embeddings.shape) == 3:\n",
    "        # (n_samples, n_genes, embedding_dim) - need to pool first or compute per gene\n",
    "        # For now, flatten gene and embedding dimensions\n",
    "        n_samples, n_genes, emb_dim = embeddings.shape\n",
    "        # Reshape to (n_samples, n_genes * emb_dim) for variance calculation\n",
    "        embeddings_flat = embeddings.reshape(n_samples, -1)\n",
    "        # Variance across samples (axis=0)\n",
    "        var_per_feature = np.var(embeddings_flat, axis=0)\n",
    "        # Reshape back to (n_genes, emb_dim) to get variance per gene\n",
    "        var_per_gene_emb = var_per_feature.reshape(n_genes, emb_dim)\n",
    "        # Average variance across embedding dimensions per gene\n",
    "        var_per_gene = np.mean(var_per_gene_emb, axis=1)\n",
    "        return var_per_gene, var_per_gene_emb\n",
    "    else:\n",
    "        # (n_samples, n_features) - assume features are already pooled\n",
    "        var_per_feature = np.var(embeddings, axis=0)\n",
    "        return var_per_feature, None\n",
    "\n",
    "\n",
    "def compute_pca_variance(embeddings, n_components=50):\n",
    "    \"\"\"\n",
    "    Compute PCA and return explained variance.\n",
    "    \"\"\"\n",
    "    # Flatten if 3D\n",
    "    if len(embeddings.shape) == 3:\n",
    "        n_samples, n_genes, emb_dim = embeddings.shape\n",
    "        embeddings_flat = embeddings.reshape(n_samples, -1)\n",
    "    else:\n",
    "        embeddings_flat = embeddings\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings_flat)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=min(n_components, embeddings_scaled.shape[1], embeddings_scaled.shape[0]-1))\n",
    "    pca.fit(embeddings_scaled)\n",
    "    \n",
    "    return {\n",
    "        \"explained_variance_ratio\": pca.explained_variance_ratio_,\n",
    "        \"explained_variance\": pca.explained_variance_,\n",
    "        \"cumulative_variance\": np.cumsum(pca.explained_variance_ratio_),\n",
    "        \"n_components\": len(pca.explained_variance_ratio_)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze variance for each configuration\n",
    "variance_results = {}\n",
    "\n",
    "for config_name, config_data in embeddings_data.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Variance Analysis: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_results = {}\n",
    "    \n",
    "    for dataset_name, data in config_data.items():\n",
    "        embeddings = data[\"embeddings\"]\n",
    "        print(f\"\\n  Dataset: {dataset_name}\")\n",
    "        print(f\"    Shape: {embeddings.shape}\")\n",
    "        \n",
    "        # Compute variance per gene\n",
    "        var_per_gene, var_per_gene_emb = compute_variance_per_gene(embeddings)\n",
    "        print(f\"    Variance per gene shape: {var_per_gene.shape}\")\n",
    "        print(f\"    Mean variance: {np.mean(var_per_gene):.4f}\")\n",
    "        print(f\"    Std variance: {np.std(var_per_gene):.4f}\")\n",
    "        \n",
    "        # Compute PCA variance\n",
    "        pca_results = compute_pca_variance(embeddings, n_components=50)\n",
    "        print(f\"    PCA components: {pca_results['n_components']}\")\n",
    "        print(f\"    First 5 PC explained variance: {pca_results['explained_variance_ratio'][:5]}\")\n",
    "        print(f\"    Cumulative variance (first 10 PCs): {pca_results['cumulative_variance'][9]:.4f}\")\n",
    "        \n",
    "        config_results[dataset_name] = {\n",
    "            \"var_per_gene\": var_per_gene,\n",
    "            \"var_per_gene_emb\": var_per_gene_emb,\n",
    "            \"pca_results\": pca_results,\n",
    "            \"embeddings_shape\": embeddings.shape\n",
    "        }\n",
    "    \n",
    "    variance_results[config_name] = config_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e331f9c",
   "metadata": {},
   "source": [
    "### Pooling Strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(embeddings):\n",
    "    \"\"\"\n",
    "    Mean pooling along gene dimension.\n",
    "    embeddings: (n_samples, n_genes, embedding_dim)\n",
    "    returns: (n_samples, embedding_dim)\n",
    "    \"\"\"\n",
    "    if len(embeddings.shape) == 3:\n",
    "        return np.mean(embeddings, axis=1)\n",
    "    else:\n",
    "        return embeddings  # Already pooled or 2D\n",
    "\n",
    "\n",
    "def max_pooling(embeddings):\n",
    "    \"\"\"\n",
    "    Max pooling along gene dimension.\n",
    "    embeddings: (n_samples, n_genes, embedding_dim)\n",
    "    returns: (n_samples, embedding_dim)\n",
    "    \"\"\"\n",
    "    if len(embeddings.shape) == 3:\n",
    "        return np.max(embeddings, axis=1)\n",
    "    else:\n",
    "        return embeddings  # Already pooled or 2D\n",
    "\n",
    "\n",
    "def sum_pooling(embeddings):\n",
    "    \"\"\"\n",
    "    Sum pooling along gene dimension.\n",
    "    embeddings: (n_samples, n_genes, embedding_dim)\n",
    "    returns: (n_samples, embedding_dim)\n",
    "    \"\"\"\n",
    "    if len(embeddings.shape) == 3:\n",
    "        return np.sum(embeddings, axis=1)\n",
    "    else:\n",
    "        return embeddings  # Already pooled or 2D\n",
    "\n",
    "\n",
    "def apply_pooling_strategies(embeddings):\n",
    "    \"\"\"\n",
    "    Apply all pooling strategies to embeddings.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if len(embeddings.shape) == 3:\n",
    "        results[\"mean\"] = mean_pooling(embeddings)\n",
    "        results[\"max\"] = max_pooling(embeddings)\n",
    "        results[\"sum\"] = sum_pooling(embeddings)\n",
    "    else:\n",
    "        # Already 2D, assume it's already pooled somehow\n",
    "        results[\"original\"] = embeddings\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pooling strategies to all embeddings\n",
    "pooling_results = {}\n",
    "\n",
    "for config_name, config_data in embeddings_data.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Pooling Analysis: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_pooling = {}\n",
    "    \n",
    "    for dataset_name, data in config_data.items():\n",
    "        embeddings = data[\"embeddings\"]\n",
    "        print(f\"\\n  Dataset: {dataset_name}\")\n",
    "        print(f\"    Original shape: {embeddings.shape}\")\n",
    "        \n",
    "        pooled = apply_pooling_strategies(embeddings)\n",
    "        \n",
    "        for strategy, pooled_emb in pooled.items():\n",
    "            print(f\"    {strategy} pooling shape: {pooled_emb.shape}\")\n",
    "        \n",
    "        config_pooling[dataset_name] = {\n",
    "            \"original\": embeddings,\n",
    "            \"pooled\": pooled\n",
    "        }\n",
    "    \n",
    "    pooling_results[config_name] = config_pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d42ac",
   "metadata": {},
   "source": [
    "### Visualization: PCA Variance Explained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA explained variance for full embeddings\n",
    "fig, axes = plt.subplots(len(available_configs), 1, figsize=(10, 4*len(available_configs)))\n",
    "if len(available_configs) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (config_name, config_results) in enumerate(variance_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for dataset_name, results in config_results.items():\n",
    "        pca_res = results[\"pca_results\"]\n",
    "        n_components = pca_res[\"n_components\"]\n",
    "        x = np.arange(1, min(21, n_components + 1))  # First 20 components\n",
    "        \n",
    "        ax.plot(x, pca_res[\"explained_variance_ratio\"][:len(x)], \n",
    "                marker='o', label=dataset_name, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Explained Variance Ratio\")\n",
    "    ax.set_title(f\"PCA Explained Variance: {config_name}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8418fa1c",
   "metadata": {},
   "source": [
    "### Visualization: Variance per Gene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variance per gene distribution\n",
    "fig, axes = plt.subplots(len(available_configs), 1, figsize=(12, 4*len(available_configs)))\n",
    "if len(available_configs) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (config_name, config_results) in enumerate(variance_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for dataset_name, results in config_results.items():\n",
    "        var_per_gene = results[\"var_per_gene\"]\n",
    "        ax.hist(var_per_gene, bins=50, alpha=0.6, label=dataset_name, density=True)\n",
    "    \n",
    "    ax.set_xlabel(\"Variance per Gene\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(f\"Variance per Gene Distribution: {config_name}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f23b4",
   "metadata": {},
   "source": [
    "### Visualization: Pooling Strategy Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da40bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pooling strategies using PCA on pooled embeddings\n",
    "for config_name, config_pooling in pooling_results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Pooling Strategy Comparison: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Compute PCA for each pooling strategy\n",
    "    pooling_pca_results = {}\n",
    "    \n",
    "    for dataset_name, data in config_pooling.items():\n",
    "        dataset_pca = {}\n",
    "        \n",
    "        for strategy, pooled_emb in data[\"pooled\"].items():\n",
    "            if len(pooled_emb.shape) == 2 and pooled_emb.shape[1] > 1:\n",
    "                pca_res = compute_pca_variance(pooled_emb, n_components=20)\n",
    "                dataset_pca[strategy] = pca_res\n",
    "        \n",
    "        pooling_pca_results[dataset_name] = dataset_pca\n",
    "    \n",
    "    # Plot comparison\n",
    "    n_datasets = len(config_pooling)\n",
    "    fig, axes = plt.subplots(1, n_datasets, figsize=(5*n_datasets, 5))\n",
    "    if n_datasets == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (dataset_name, dataset_pca) in zip(axes, pooling_pca_results.items()):\n",
    "        for strategy, pca_res in dataset_pca.items():\n",
    "            n_comp = min(20, pca_res[\"n_components\"])\n",
    "            x = np.arange(1, n_comp + 1)\n",
    "            ax.plot(x, pca_res[\"explained_variance_ratio\"][:n_comp], \n",
    "                   marker='o', label=strategy, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel(\"Principal Component\")\n",
    "        ax.set_ylabel(\"Explained Variance Ratio\")\n",
    "        ax.set_title(f\"{dataset_name}\\nPooling Strategy Comparison\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f\"PCA Comparison: {config_name}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae680694",
   "metadata": {},
   "source": [
    "### Visualization: UMAP Clustering for Pooling Strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b21cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import umap\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    # UMAP visualization for each pooling strategy\n",
    "    for config_name, config_pooling in pooling_results.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"UMAP Clustering: {config_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for dataset_name, data in config_pooling.items():\n",
    "            metadata = embeddings_data[config_name][dataset_name][\"metadata\"]\n",
    "            \n",
    "            # Get status labels if available\n",
    "            if metadata is not None:\n",
    "                status_cols = [\"takao_status\", \"status\", \"condition\", \"group\"]\n",
    "                status_col = None\n",
    "                for col in status_cols:\n",
    "                    if col in metadata.columns:\n",
    "                        status_col = col\n",
    "                        break\n",
    "                \n",
    "                if status_col:\n",
    "                    labels = metadata[status_col].values\n",
    "                else:\n",
    "                    labels = None\n",
    "            else:\n",
    "                labels = None\n",
    "            \n",
    "            # Plot UMAP for each pooling strategy\n",
    "            n_strategies = len([s for s in data[\"pooled\"].keys() if len(data[\"pooled\"][s].shape) == 2])\n",
    "            fig, axes = plt.subplots(1, n_strategies, figsize=(5*n_strategies, 5))\n",
    "            if n_strategies == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            ax_idx = 0\n",
    "            for strategy, pooled_emb in data[\"pooled\"].items():\n",
    "                if len(pooled_emb.shape) == 2 and pooled_emb.shape[1] > 2:\n",
    "                    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=min(15, len(pooled_emb)-1))\n",
    "                    embedding_2d = reducer.fit_transform(pooled_emb)\n",
    "                    \n",
    "                    ax = axes[ax_idx]\n",
    "                    if labels is not None:\n",
    "                        unique_labels = np.unique(labels)\n",
    "                        for label in unique_labels:\n",
    "                            mask = labels == label\n",
    "                            ax.scatter(embedding_2d[mask, 0], embedding_2d[mask, 1], \n",
    "                                     label=str(label), alpha=0.6, s=50)\n",
    "                        ax.legend()\n",
    "                    else:\n",
    "                        ax.scatter(embedding_2d[:, 0], embedding_2d[:, 1], alpha=0.6, s=50)\n",
    "                    \n",
    "                    ax.set_xlabel(\"UMAP 1\")\n",
    "                    ax.set_ylabel(\"UMAP 2\")\n",
    "                    ax.set_title(f\"{strategy} pooling\")\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    ax_idx += 1\n",
    "            \n",
    "            plt.suptitle(f\"{dataset_name} - UMAP Clustering\", fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"UMAP not available. Install with: pip install umap-learn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e297d2",
   "metadata": {},
   "source": [
    "### Summary Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0de47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table comparing pooling strategies\n",
    "summary_rows = []\n",
    "\n",
    "for config_name, config_pooling in pooling_results.items():\n",
    "    for dataset_name, data in config_pooling.items():\n",
    "        original_shape = data[\"original\"].shape\n",
    "        \n",
    "        for strategy, pooled_emb in data[\"pooled\"].items():\n",
    "            if len(pooled_emb.shape) == 2:\n",
    "                # Compute statistics\n",
    "                pca_res = compute_pca_variance(pooled_emb, n_components=10)\n",
    "                \n",
    "                summary_rows.append({\n",
    "                    \"config\": config_name,\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"original_shape\": str(original_shape),\n",
    "                    \"pooled_shape\": str(pooled_emb.shape),\n",
    "                    \"mean\": np.mean(pooled_emb),\n",
    "                    \"std\": np.std(pooled_emb),\n",
    "                    \"pca_variance_1pc\": pca_res[\"explained_variance_ratio\"][0] if len(pca_res[\"explained_variance_ratio\"]) > 0 else np.nan,\n",
    "                    \"pca_variance_10pc\": pca_res[\"cumulative_variance\"][9] if len(pca_res[\"cumulative_variance\"]) > 9 else np.nan,\n",
    "                })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"Summary Statistics for Pooling Strategies:\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
